{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Why we need  machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples of tasks best solved by learning\n",
    "- Recognizing patterns\n",
    "\t- objects in real scenes\n",
    "\t- facial identities or facial expressions\n",
    "\t- spoken words\n",
    "- Recognizing anomalies\n",
    "\t- unusual sequences of credit card transactions\n",
    "\t- unusual patterns of sensors reading a nuclear power plant\n",
    "- Prediction\n",
    "\t- future stock prices or currency exchanges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 What are neural networks?\n",
    "(some examples of biological neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Some simple models of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idealized neurons\n",
    "- To model thins we have to idealize them (e.g. atoms)\n",
    "\t- Idealization removes complicated details that are not essential for understanding the main principles\n",
    "\t- It allow us to apply matematics and to make analogies to other, familiar systems\n",
    "\t- Once we understand the basic principles, its easy to add complexity to make the model more faithful\n",
    "- It is often worth  understanding models that are known to be wrong (but we must not forget that they are wrong!)\n",
    "\t- E.g neurons that communicate real values rather than discrete spikes of activity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear neurons\n",
    "- These are simple but computationally limited\n",
    "\t- if we can make then learn we *may* get insight into more complicated neurons\n",
    "\t- $y = b + \\displaystyle{\\sum_{i}x_iw_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary threshold neurons\n",
    "- McCulloch-Pitts (1943): *influence by Von Neumann*\n",
    "\t- First compute a weighted sum of inputs\n",
    "\t- Then send out fixed size spike of activity if the weighted sum exceeds a threshold\n",
    "\t- McCulloch and Pitts thought that each spike is like the truth value of a proposition and each neuron combines truth values to compute the truth value of another proposition.\n",
    "- There are two equivalent ways to write the equations for a binary threshold neuron:\n",
    "\n",
    "| |  | |\n",
    "|----------|----------|---------|\n",
    "| $z = \\displaystyle{\\sum_{i}x_iw_i}$ | $ \\theta = -b $ | $z = \\displaystyle{b + \\sum_{i}x_iw_i}$ |\n",
    "| $ y = \\begin{cases} 1 & \\quad \\text{if } z \\geq \\theta \\\\ 0 & \\quad \\text{otherwise} \\end{cases} $ | | $ y = \\begin{cases} 1 & \\quad \\text{if } z \\geq 0 \\\\ 0 & \\quad \\text{otherwise} \\end{cases} $ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectifier Linear Neurons (sometimes called threshold neurons)\n",
    "They compute a *linear* weighted sum of their inputs.\n",
    "The output is a *non-linear* function of the total input\n",
    "\n",
    "$z = b + \\displaystyle{\\sum_{i}x_iw_i} $\n",
    "\n",
    "$ y = \\begin{cases} z & \\quad \\text{if } z = 0 \\\\ 0 & \\quad \\text{otherwise} \\end{cases} $ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid neurons\n",
    "- These give a real-valued output that is a smooth and bounded function of their total input\n",
    "\t- Typically they use the use the logistic function\n",
    "\t- They have nice derivatives which make learning easy\n",
    "\n",
    "$ z = b + \\displaystyle{\\sum_{i}x_iw_i} $\n",
    "\n",
    "$ y = \\frac{1}{1 + e^{-z}} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic binary neurons\n",
    "\n",
    "- These use the same equations as logistics units:\n",
    " - But they treat the output of the logistic as the *probability* of producing a spike in a short time window\n",
    "- We can do a similar trick for rectifier linear units:\n",
    "\t- The output is treated as poisson rate for spikes (????)\n",
    "\n",
    "$z = b + \\displaystyle{\\sum_{i}x_iw_i} $\n",
    "\n",
    "$ p(s=1) = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "![](http://image.slidesharecdn.com/2-typesofneurons-151231003446/95/neural-networks-types-of-neurons-13-638.jpg?cb=1451522241) (revisit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 A simple example of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Three types of learning\n",
    "### Types of learning tasks\n",
    "- Supervised learning\n",
    "\t- Learn to predict an output when given an input vector\n",
    "- Reinforced learning\n",
    "\t- Learn to select an action to maximized payoff\n",
    "- Unsupervised learning\n",
    "\t- Discover a good internal representation of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two types of supervised learning\n",
    "- Each training consists of an input vector $x$ and  target output $t$\n",
    "- _Regression:_ The target output is a real number or a whole vector of real numbers.\n",
    "\t- The price of a stock in 6 months time.\n",
    "\t- The temperature at noon tomorrow.\n",
    "- _Classification:_ The target output is a class label\n",
    "\t- The simplest case is a choice between 1 and 0\n",
    "\t- We can also have multiple alternative labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How supervised learning typically works\n",
    "- We start choosing a _model class_: $y = f(x;W)$\n",
    "\t- A model-class, $f$, is a way of using some numerical parameters, $W$, to map each input vector, $x$, into a predicated output $y$.\n",
    "- Learning usually means adjusting the parameters to reduce the discrepancy between the target output, $t$, on each training case and the actual output, $y$, produced by the model.\n",
    "\t- For regression, $\\frac{1}{2}(y - t)^2$ is often a sensible measure of the discrepancy\n",
    "\t- For classification there are other measures that are generally more sensible (they also work better)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforced Learning\n",
    "- In reinforced learning, the output is an action or sequence of actions and the only supervisory signal is an occasional scalar reward\n",
    "\t- The goal in selecting each action is to maximize the expected sum of future rewards\n",
    "\t- We usually use a discount factor for delayed rewards so that we don't have to look too far in the future\n",
    "- Reinforced learning is difficult:\n",
    "\t- The rewards are typically delayed so its hard to know where we went wrong (or right)\n",
    "\t- A scalar reward dos not supply much information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "- For about 40 years, unsupervised learning was largely ignored byt the machine learning community\n",
    "\t- Some widely used definitions of machine learning actually excluded it.\n",
    "\t- Many researchers thought that clustering was the only form of unsupervised learning\n",
    "- It is hard to say what the aim of unsupervised learning is\n",
    "\t- One major aim is to create an internal representation of the input that is useful for subsequent supervised or reinforcement learning\n",
    "\t- You can comppute the distance to a surface by using the disparity between two images. But you don't want to learn to comppute disparities by stubbing your toe thousands of times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other goals for unsupervised learning\n",
    "- It provides a compact, low-dimensional representation of the input\n",
    "\t- High-dimensional inputs typically live on or near a low-dimensional manifold (or several such manifolds)\n",
    "- It provides an economical high-dimensional representation of the input in terms of learned features.\n",
    "\t- Binay features are economical.\n",
    "\t- So are real-valued features that are nearly all zero.\n",
    "- If finds sensible clusters in the input.\n",
    "\t- This is an example of a *very* sparse code in which only one of the features is non-zero.\n",
    "\n",
    "_Note:_ The goal of unsupervised learning would be to transform inputs such as these binary images into another representation that makes them easier to deal with in some way. For example, making the digits easier to classify or discovering some interesting semantic properties such as dictionary or strokes that can be used for form each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
